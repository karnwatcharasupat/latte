{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karnwatcharasupat/latte/blob/main/examples/morphomnist/morphomnist-torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SEjpqcL0FHh"
      },
      "source": [
        "# Using Latte with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4gm_C-N7MBz"
      },
      "source": [
        "This example notebook demonstrates the use of [Latte](https://github.com/karnwatcharasupat/latte) with vanilla [PyTorch](https://pytorch.org/) (without Lightning). \n",
        "\n",
        "The code in this notebook is adapted from the [AR-VAE](https://github.com/ashispati/ar-vae) implementation:\n",
        "> A. Pati and A. Lerch, Attribute-based regularization of latent spaces for variational auto-encoders. Neural Computing & Applications, 33, 4429â€“4444 (2021). https://doi.org/10.1007/s00521-020-05270-2\n",
        "\n",
        "\n",
        "For this notebook, we will be using the [Morpho-MNIST](https://github.com/dccastro/Morpho-MNIST) dataset which is a disentanglement dataset built on the usual MNIST dataset.\n",
        "\n",
        "**Before you begin, please turn on GPU accelerator at `Runtime > Change runtime type > Hardware accelerator > GPU`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R41hFIOfiiE"
      },
      "outputs": [],
      "source": [
        "HOME = '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v4buxl60Lfv"
      },
      "source": [
        "## Installing Latte and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY1z4qYe0D5i"
      },
      "outputs": [],
      "source": [
        "# This command automatically install PyTorch and TorchMetrics.\n",
        "# For users with existing pytorch>=1.3.1 and torchmetrics>=0.2.0 installation, \n",
        "#   use `pip install latte-metrics` with no extras\n",
        "!pip install -q latte-metrics[pytorch]   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbZfmlo18PXg"
      },
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXx5z1SK1GCl"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nmis2eO1gIt"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "export DSET_PATH=\"/content/dataset\" \n",
        "mkdir -p $DSET_PATH\n",
        "gdown --id \"1fFGJW0IHoBmLuD6CEKCB8jz3Y5LJ5Duk\" -O $DSET_PATH/morphomnist.zip\n",
        "unzip -o \"$DSET_PATH/morphomnist.zip\" -d $DSET_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCGLIL6K5Bkq"
      },
      "source": [
        "### Cloning Morpho-MNIST measurement code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQSicjdF0b57"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/dccastro/Morpho-MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CweifwUB8JF_"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "sys.path.append(os.path.join(HOME, 'Morpho-MNIST'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGfmt5hR8JnF"
      },
      "source": [
        "### Creating dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eiisEft71kO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from morphomnist import io, morpho\n",
        "\n",
        "class MorphoMnistDataset():\n",
        "\n",
        "    def __init__(self, root_dir=os.path.join(HOME, 'dataset/global')):\n",
        "        super().__init__()\n",
        "        self.kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "        self.root_dir = root_dir\n",
        "        self.data_path_str = \"-images-idx3-ubyte.gz\"\n",
        "        self.label_path_str = \"-labels-idx1-ubyte.gz\"\n",
        "        self.morpho_path_str = \"-morpho.csv\"\n",
        "\n",
        "        self.train_dataset = self._create_dataset(dataset_type=\"train\")\n",
        "        self.val_dataset = self._create_dataset(dataset_type=\"t10k\")\n",
        "\n",
        "    def dataloaders(self, batch_size, limit=None):\n",
        "        \n",
        "        train_dl = DataLoader(\n",
        "            self.train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            **self.kwargs\n",
        "        )\n",
        "        \n",
        "        val_dl = DataLoader(\n",
        "            self.val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "        )\n",
        "        return train_dl, val_dl\n",
        "\n",
        "    def _create_dataset(self, dataset_type):\n",
        "        data_path = os.path.join(\n",
        "            self.root_dir,\n",
        "            dataset_type + self.data_path_str\n",
        "        )\n",
        "        morpho_path = os.path.join(\n",
        "            self.root_dir,\n",
        "            dataset_type + self.morpho_path_str\n",
        "        )\n",
        "        images = io.load_idx(data_path)\n",
        "        images = np.expand_dims(images, axis=1).astype('float32') / 255.0\n",
        "        morpho_labels = pd.read_csv(morpho_path).values.astype('float32')[:, 3:]\n",
        "        dataset = TensorDataset(\n",
        "            torch.from_numpy(images),\n",
        "            torch.from_numpy(morpho_labels)\n",
        "        )\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsHMwY__5TNa"
      },
      "source": [
        "## Creating a simple AR-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuMVZHRqfiiU"
      },
      "source": [
        "### Defining the Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI9EBkI-AqrT"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "\n",
        "def ar_signed_loss(z, a, factor=10.0):\n",
        "\n",
        "    n_attr = a.shape[-1]\n",
        "\n",
        "    # compute latent distance matrix\n",
        "    lc_dist_mat = z[:, None, :n_attr] - z[None, :, :n_attr]\n",
        "\n",
        "    # compute attribute distance matrix\n",
        "    attribute_dist_mat = a[:, None, ...] - a[None, :, :]\n",
        "\n",
        "    # compute regularization loss\n",
        "    lc_tanh = torch.tanh(lc_dist_mat * factor)\n",
        "    attribute_sign = torch.sign(attribute_dist_mat)\n",
        "    batch_size = z.shape[0]\n",
        "    ar_loss = F.l1_loss(lc_tanh, attribute_sign.float(), reduction='sum')/(batch_size ** 2 - batch_size)\n",
        "\n",
        "    return ar_loss\n",
        "\n",
        "def compute_loss(x, xhat, zd, z0, z, a, beta=1.0, gamma=1.0):\n",
        "\n",
        "    recon_loss = F.mse_loss(x, torch.sigmoid(xhat), reduction='sum')/z.shape[0]\n",
        "\n",
        "    kld_loss = distributions.kl.kl_divergence(zd, z0).sum(-1).mean()\n",
        "\n",
        "    ar_loss = ar_signed_loss(z, a)\n",
        "\n",
        "    return {\n",
        "        'loss': recon_loss + beta * kld_loss + gamma * ar_loss,\n",
        "        'recon_loss': recon_loss,\n",
        "        'kld_loss': kld_loss,\n",
        "        'ar_loss': ar_loss\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ELr5tXfiiW"
      },
      "source": [
        "### Defining base VAE class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alAggbUd5VBk"
      },
      "outputs": [],
      "source": [
        "from torch import nn, distributions\n",
        "\n",
        "class ImageVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_size = 784\n",
        "        self.z_dim = 16\n",
        "        self.inter_dim = 19\n",
        "        self.enc_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 4, 1),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(64, 64, 4, 1),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Conv2d(64, 8, 4, 1),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "        self.enc_lin = nn.Sequential(\n",
        "            nn.Linear(2888, 256),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.enc_mean = nn.Linear(256, self.z_dim)\n",
        "        self.enc_log_std = nn.Linear(256, self.z_dim)\n",
        "        self.dec_lin = nn.Sequential(\n",
        "            nn.Linear(self.z_dim, 256),\n",
        "            nn.SELU(),\n",
        "            nn.Linear(256, 2888),\n",
        "            nn.SELU()\n",
        "        )\n",
        "        self.dec_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(8, 64, 4, 1),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ConvTranspose2d(64, 64, 4, 1),\n",
        "            nn.SELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ConvTranspose2d(64, 1, 4, 1),\n",
        "        )\n",
        "\n",
        "        self.xavier_initialization()\n",
        "\n",
        "    def xavier_initialization(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.xavier_normal_(param)\n",
        "\n",
        "    def encode(self, x):\n",
        "        hidden = self.enc_conv(x)\n",
        "        hidden = hidden.view(x.size(0), -1)\n",
        "        hidden = self.enc_lin(hidden)\n",
        "        z_mean = self.enc_mean(hidden)\n",
        "        z_log_std = self.enc_log_std(hidden)\n",
        "        z_distribution = distributions.Normal(loc=z_mean, scale=torch.exp(z_log_std) + 1e-16)\n",
        "        return z_distribution\n",
        "\n",
        "    def decode(self, z):\n",
        "        hidden = self.dec_lin(z)\n",
        "        hidden = hidden.view(z.size(0), -1, self.inter_dim, self.inter_dim)\n",
        "        hidden = self.dec_conv(hidden)\n",
        "        return hidden\n",
        "\n",
        "    def reparametrize(self, z_dist):\n",
        "        # sample from distribution\n",
        "        z_tilde = z_dist.rsample()\n",
        "\n",
        "        # compute prior\n",
        "        prior_dist = torch.distributions.Normal(\n",
        "            loc=torch.zeros_like(z_dist.loc),\n",
        "            scale=torch.ones_like(z_dist.scale)\n",
        "        )\n",
        "        return z_tilde, prior_dist\n",
        "\n",
        "    def forward(self, x):\n",
        "        # compute distribution using encoder\n",
        "        z_dist = self.encode(x)\n",
        "\n",
        "        # reparametrize\n",
        "        z_tilde, prior_dist = self.reparametrize(z_dist)\n",
        "\n",
        "        # compute output of decoding layer\n",
        "        output = self.decode(z_tilde).view(x.size())\n",
        "\n",
        "        return output, z_dist, prior_dist, z_tilde\n",
        "    \n",
        "    def interpolate(self, x, dz):\n",
        "        # compute distribution using encoder\n",
        "        z_dist = self.encode(x)\n",
        "\n",
        "        # reparametrize\n",
        "        z_tilde, prior_dist = self.reparametrize(z_dist)\n",
        "\n",
        "        # compute output of decoding layer\n",
        "        if dz.ndim > 1:\n",
        "            output = []\n",
        "            for i in range(dz.shape[-1]):\n",
        "                output.append(self.decode(z_tilde + dz[None, :, i]).view(x.size()))\n",
        "            output = torch.stack(output, axis=-1)\n",
        "        else:\n",
        "            output = self.decode(z_tilde + dz[None, :]).view(x.size())\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DvigofKfiiY"
      },
      "source": [
        "### Defining a Metric Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hegle94PfiiY"
      },
      "outputs": [],
      "source": [
        "from latte.metrics.torch.bundles import DependencyAwareMutualInformationBundle\n",
        "\n",
        "dami = DependencyAwareMutualInformationBundle(reg_dim=range(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6803GEY-RQJE"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "For your convenience, we have prepared pretrained weights for the model. This notebook will only train for one more epoch as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXhFTc2QfiiZ"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/karnwatcharasupat/latte/raw/main/examples/morphomnist/weights/morphomnist-torch-weights.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hvsM1wP6lqk"
      },
      "outputs": [],
      "source": [
        "import latte\n",
        "\n",
        "latte.seed(42) \n",
        "# there is no need for this\n",
        "# this is just to demonstrate that you can manually set a seed\n",
        "# Latte uses seed=42 by default anyway\n",
        "\n",
        "model = ImageVAE()\n",
        "model.load_state_dict(torch.load(os.path.join(HOME, 'morphomnist-torch-weights.pth')))\n",
        "model = model.cuda()\n",
        "\n",
        "dataset = MorphoMnistDataset()\n",
        "train_dl, val_dl = dataset.dataloaders(batch_size=32, limit=0.1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mblhS28n97z_"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "n_batch = len(train_dl)\n",
        "n_val_batch = len(val_dl)\n",
        "\n",
        "postfix = {}\n",
        "\n",
        "for epoch_index in range(1):\n",
        "\n",
        "    model.train()\n",
        "    with tqdm(total=int(0.1*n_batch)) as prog_bar:\n",
        "        for i, data in enumerate(train_dl):\n",
        "            prog_bar.update()\n",
        "            \n",
        "            inputs, attributes = data\n",
        "            inputs = inputs.cuda()\n",
        "            \n",
        "            model.zero_grad()\n",
        "\n",
        "            recon, z_dist, prior_dist, z_tilde = model(inputs)\n",
        "            \n",
        "            loss = compute_loss(\n",
        "                inputs, recon, z_dist, prior_dist, z_tilde, attributes.cuda()\n",
        "            )\n",
        "\n",
        "            loss['loss'].backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            postfix.update({f\"train/{k}\": loss[k].detach().cpu().numpy() for k in loss})\n",
        "            \n",
        "            # for training, we calculate the metrics every 8 batches\n",
        "            if i % 8 == 0:\n",
        "\n",
        "                # Latte automatically move all data to CPU\n",
        "                # There is no need to call `.cpu()` here.\n",
        "                dami.reset()\n",
        "                dami.update(z_tilde, attributes)\n",
        "                train_metrics = dami.compute()\n",
        "                \n",
        "                # We only put the mean metrics over the attributes here on the progress bar for demonstration\n",
        "                postfix.update({f\"train/{k}\": train_metrics[k].mean().detach().cpu().numpy() for k in train_metrics})\n",
        "\n",
        "            prog_bar.set_postfix(postfix)\n",
        "            \n",
        "            if i > 0.1 * n_batch:\n",
        "                break\n",
        "\n",
        "        # reset cache for validation loop\n",
        "        dami.reset()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        \n",
        "    with tqdm(total=int(0.1*n_val_batch)) as prog_bar:\n",
        "        \n",
        "        for j, data in enumerate(val_dl):\n",
        "            \n",
        "            prog_bar.update()\n",
        "\n",
        "            inputs, attributes = data\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "            recon, z_dist, prior_dist, z_tilde = model(inputs)\n",
        "\n",
        "            loss = compute_loss(\n",
        "                inputs, recon, z_dist, prior_dist, z_tilde, attributes.cuda()\n",
        "            )\n",
        "\n",
        "            val_loss += loss['loss']\n",
        "\n",
        "            # use the entire validation set to compute metrics this time\n",
        "            dami.update(z_tilde, attributes)\n",
        "            \n",
        "            if j > 0.1 * n_val_batch:\n",
        "                break\n",
        "\n",
        "        # only compute once at the end of the validation loop \n",
        "        # using all validation batches\n",
        "        val_metrics = dami.compute()\n",
        "\n",
        "        print(f\"Validation loss: {val_loss/len(val_dl):3.2g}\")\n",
        "        for metric in val_metrics:\n",
        "            print(f\"Validation {metric}: {val_metrics[metric].numpy().mean():.3g}\")\n",
        "\n",
        "        # reset cache for the next train loop\n",
        "        dami.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fLl7Co-fiib"
      },
      "source": [
        "## Visualizing the outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRQQ-KpYfiic"
      },
      "outputs": [],
      "source": [
        "atttribute_dict = {\n",
        "    \"thickness\": 0,\n",
        "    \"slant\": 1,\n",
        "    \"width\": 2,\n",
        "    \"height\": 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWoO9Rvifiic"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "\n",
        "def interpolate_and_display(model, attribute):\n",
        "    model.eval()\n",
        "    model = model.cuda()\n",
        "    \n",
        "    f = plt.figure(figsize=(16, 16))\n",
        "    ax = ImageGrid(\n",
        "        f, 111,  # similar to subplot(111)\n",
        "        nrows_ncols=(11, 8),  # creates 2x2 grid of axes\n",
        "        axes_pad=0.1,  # pad between axes in inch.\n",
        "    )\n",
        "\n",
        "    inputs, _ = dataset.val_dataset[torch.randint(10000, (8,))]\n",
        "\n",
        "    dz = torch.zeros((16, 10))\n",
        "    dz[atttribute_dict[attribute], :] = torch.linspace(-2.0, 2.0, 10)\n",
        "\n",
        "    gen = model.interpolate(inputs.cuda(), dz.cuda())\n",
        "    gen = torch.sigmoid(gen)\n",
        "\n",
        "    for i in range(8):\n",
        "        ax[i].imshow(inputs.detach().cpu()[i, 0, :, :], cmap='summer')\n",
        "        for j in range(10):\n",
        "            ax[(j+1)*8+i].imshow(gen.detach().cpu()[i, 0, :, :, j], cmap='gray', vmin=0, vmax=1)\n",
        "    \n",
        "    for i in range(8*11):\n",
        "        ax[i].axis('off')\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYMb46xafiid"
      },
      "outputs": [],
      "source": [
        "interpolate_and_display(model, 'thickness')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9jo4s1bfiid"
      },
      "outputs": [],
      "source": [
        "interpolate_and_display(model, 'slant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EytOj4NZfiid"
      },
      "outputs": [],
      "source": [
        "interpolate_and_display(model, 'width')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "morphomnist-torch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SEjpqcL0FHh"
   },
   "source": [
    "# Using Latte with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4gm_C-N7MBz"
   },
   "source": [
    "This example notebook demonstrates the use of [Latte](https://github.com/karnwatcharasupat/latte) with vanilla [PyTorch](https://pytorch.org/) (without Lightning). \n",
    "\n",
    "The code in this notebook is adapted from the [AR-VAE](https://github.com/ashispati/ar-vae) implementation:\n",
    "> A. Pati and A. Lerch, Attribute-based regularization of latent spaces for variational auto-encoders. Neural Computing & Applications, 33, 4429â€“4444 (2021). https://doi.org/10.1007/s00521-020-05270-2\n",
    "\n",
    "\n",
    "For this notebook, we will be using the [Morpho-MNIST](https://github.com/dccastro/Morpho-MNIST) dataset which is a disentanglement dataset built on the usual MNIST dataset.\n",
    "\n",
    "**Before you begin, please turn on GPU accelerator at `Runtime > Change runtime type > Hardware accelerator > GPU`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v4buxl60Lfv"
   },
   "source": [
    "## Installing Latte and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY1z4qYe0D5i"
   },
   "outputs": [],
   "source": [
    "# This command automatically install PyTorch and TorchMetrics.\n",
    "# For users with existing pytorch>=1.3.1 and torchmetrics>=0.2.0 installation, \n",
    "#   use `pip install latte-metrics` with no extras\n",
    "!pip install -q latte-metrics[pytorch]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbZfmlo18PXg"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXx5z1SK1GCl"
   },
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nmis2eO1gIt",
    "outputId": "bd92a03d-ad63-4099-ec62-65c25a760b11"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "export DSET_PATH=\"/content/dataset\" \n",
    "mkdir -p $DSET_PATH\n",
    "gdown --id \"1fFGJW0IHoBmLuD6CEKCB8jz3Y5LJ5Duk\" -O $DSET_PATH/morphomnist.zip\n",
    "unzip -o \"$DSET_PATH/morphomnist.zip\" -d $DSET_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCGLIL6K5Bkq"
   },
   "source": [
    "### Cloning Morpho-MNIST measurement code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQSicjdF0b57",
    "outputId": "f52f2c57-2bd4-4134-ef71-11f50378187e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/dccastro/Morpho-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CweifwUB8JF_"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(HOME, 'Morpho-MNIST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# sys.path.append(os.path.expanduser('~/Morpho-MNIST'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGfmt5hR8JnF"
   },
   "source": [
    "### Creating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eiisEft71kO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from morphomnist import io, morpho\n",
    "\n",
    "class MorphoMnistDataset():\n",
    "\n",
    "    def __init__(self, root_dir=os.path.join(HOME, 'dataset/global')):\n",
    "        super().__init__()\n",
    "        self.kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "        self.root_dir = root_dir\n",
    "        self.data_path_str = \"-images-idx3-ubyte.gz\"\n",
    "        self.label_path_str = \"-labels-idx1-ubyte.gz\"\n",
    "        self.morpho_path_str = \"-morpho.csv\"\n",
    "\n",
    "        self.train_dataset = self._create_dataset(dataset_type=\"train\")\n",
    "        self.val_dataset = self._create_dataset(dataset_type=\"t10k\")\n",
    "\n",
    "    def dataloaders(self, batch_size, limit=None):\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            **self.kwargs\n",
    "        )\n",
    "        \n",
    "        val_dl = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return train_dl, val_dl\n",
    "\n",
    "    def _create_dataset(self, dataset_type):\n",
    "        data_path = os.path.join(\n",
    "            self.root_dir,\n",
    "            dataset_type + self.data_path_str\n",
    "        )\n",
    "        morpho_path = os.path.join(\n",
    "            self.root_dir,\n",
    "            dataset_type + self.morpho_path_str\n",
    "        )\n",
    "        images = io.load_idx(data_path)\n",
    "        images = np.expand_dims(images, axis=1).astype('float32') / 255.0\n",
    "        morpho_labels = pd.read_csv(morpho_path).values.astype('float32')[:, 3:]\n",
    "        dataset = TensorDataset(\n",
    "            torch.from_numpy(images),\n",
    "            torch.from_numpy(morpho_labels)\n",
    "        )\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsHMwY__5TNa"
   },
   "source": [
    "## Creating a simple AR-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sI9EBkI-AqrT"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def ar_signed_loss(z, a, factor=10.0):\n",
    "\n",
    "    n_attr = a.shape[-1]\n",
    "\n",
    "    # compute latent distance matrix\n",
    "    lc_dist_mat = z[:, None, :n_attr] - z[None, :, :n_attr]\n",
    "\n",
    "    # compute attribute distance matrix\n",
    "    attribute_dist_mat = a[:, None, ...] - a[None, :, :]\n",
    "\n",
    "    # compute regularization loss\n",
    "    lc_tanh = torch.tanh(lc_dist_mat * factor)\n",
    "    attribute_sign = torch.sign(attribute_dist_mat)\n",
    "    batch_size = z.shape[0]\n",
    "    ar_loss = F.l1_loss(lc_tanh, attribute_sign.float(), reduction='sum')/(batch_size ** 2 - batch_size)\n",
    "\n",
    "    return ar_loss\n",
    "\n",
    "def compute_loss(x, xhat, zd, z0, z, a, beta=1.0, gamma=1.0):\n",
    "\n",
    "    recon_loss = F.mse_loss(x, torch.sigmoid(xhat), reduction='sum')/z.shape[0]\n",
    "\n",
    "    kld_loss = distributions.kl.kl_divergence(zd, z0).sum(-1).mean()\n",
    "\n",
    "    ar_loss = ar_signed_loss(z, a)\n",
    "\n",
    "    return {\n",
    "        'loss': recon_loss + beta * kld_loss + gamma * ar_loss,\n",
    "        'recon_loss': recon_loss,\n",
    "        'kld_loss': kld_loss,\n",
    "        'ar_loss': ar_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining base VAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alAggbUd5VBk"
   },
   "outputs": [],
   "source": [
    "from torch import nn, distributions\n",
    "\n",
    "class ImageVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 784\n",
    "        self.z_dim = 16\n",
    "        self.inter_dim = 19\n",
    "        self.enc_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 1),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(64, 64, 4, 1),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(64, 8, 4, 1),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.enc_lin = nn.Sequential(\n",
    "            nn.Linear(2888, 256),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.enc_mean = nn.Linear(256, self.z_dim)\n",
    "        self.enc_log_std = nn.Linear(256, self.z_dim)\n",
    "        self.dec_lin = nn.Sequential(\n",
    "            nn.Linear(self.z_dim, 256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256, 2888),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.dec_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 64, 4, 1),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(64, 64, 4, 1),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ConvTranspose2d(64, 1, 4, 1),\n",
    "        )\n",
    "\n",
    "        self.xavier_initialization()\n",
    "\n",
    "    def xavier_initialization(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_normal_(param)\n",
    "\n",
    "    def encode(self, x):\n",
    "        hidden = self.enc_conv(x)\n",
    "        hidden = hidden.view(x.size(0), -1)\n",
    "        hidden = self.enc_lin(hidden)\n",
    "        z_mean = self.enc_mean(hidden)\n",
    "        z_log_std = self.enc_log_std(hidden)\n",
    "        z_distribution = distributions.Normal(loc=z_mean, scale=torch.exp(z_log_std) + 1e-16)\n",
    "        return z_distribution\n",
    "\n",
    "    def decode(self, z):\n",
    "        hidden = self.dec_lin(z)\n",
    "        hidden = hidden.view(z.size(0), -1, self.inter_dim, self.inter_dim)\n",
    "        hidden = self.dec_conv(hidden)\n",
    "        return hidden\n",
    "\n",
    "    def reparametrize(self, z_dist):\n",
    "        # sample from distribution\n",
    "        z_tilde = z_dist.rsample()\n",
    "\n",
    "        # compute prior\n",
    "        prior_dist = torch.distributions.Normal(\n",
    "            loc=torch.zeros_like(z_dist.loc),\n",
    "            scale=torch.ones_like(z_dist.scale)\n",
    "        )\n",
    "        return z_tilde, prior_dist\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute distribution using encoder\n",
    "        z_dist = self.encode(x)\n",
    "\n",
    "        # reparametrize\n",
    "        z_tilde, prior_dist = self.reparametrize(z_dist)\n",
    "\n",
    "        # compute output of decoding layer\n",
    "        output = self.decode(z_tilde).view(x.size())\n",
    "\n",
    "        return output, z_dist, prior_dist, z_tilde\n",
    "    \n",
    "    def interpolate(self, x, dz):\n",
    "        # compute distribution using encoder\n",
    "        z_dist = self.encode(x)\n",
    "\n",
    "        # reparametrize\n",
    "        z_tilde, prior_dist = self.reparametrize(z_dist)\n",
    "\n",
    "        # compute output of decoding layer\n",
    "        if dz.ndim > 1:\n",
    "            output = []\n",
    "            for i in range(dz.shape[-1]):\n",
    "                output.append(self.decode(z_tilde + dz[None, :, i]).view(x.size()))\n",
    "            output = torch.stack(output, axis=-1)\n",
    "        else:\n",
    "            output = self.decode(z_tilde + dz[None, :]).view(x.size())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Metric Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latte.metrics.torch.bundles import DependencyAwareMutualInformationBundle\n",
    "\n",
    "dami = DependencyAwareMutualInformationBundle(reg_dim=range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6803GEY-RQJE"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "For your convenience, we have prepared pretrained weights for the model. This notebook will only train for one more epoch as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/karnwatcharasupat/latte/raw/main/examples/morphomnist/weights/morphomnist-torch-weights.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hvsM1wP6lqk"
   },
   "outputs": [],
   "source": [
    "import latte\n",
    "\n",
    "latte.seed(42) \n",
    "# there is no need for this\n",
    "# this is just to demonstrate that you can manually set a seed\n",
    "# Latte uses seed=42 by default anyway\n",
    "\n",
    "model = ImageVAE()\n",
    "model.load_state_dict(torch.load(os.path.join(HOME, 'morphomnist-torch-weights.pth')))\n",
    "model = model.cuda()\n",
    "\n",
    "dataset = MorphoMnistDataset(root_dir='/home/karn/data/global')\n",
    "train_dl, val_dl = dataset.data_loaders(batch_size=32, limit=0.1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445,
     "referenced_widgets": [
      "6a399fb992244e4f95632df22228db02",
      "26be1cc09bba403491747d0a20140373",
      "5970bb50ff844ec7acfd02ff68db8010",
      "47c1be5335ad4f1e97b81dddda3a9d79",
      "84c042f3ceca4495be006145d97c81cf",
      "2fa0e4d346ea48c3a520ca8d9e014703",
      "7fe4080d475a41319b3ea064267ab37b",
      "6a2a135150974910b197f7b5d501a0f4",
      "53ad6097a36c4f168993fa7e7c9c0256",
      "42e75f19949c48b1b0fd5ef711481334",
      "9f85f8eec6ae4419873149fe830fa6a4"
     ]
    },
    "id": "mblhS28n97z_",
    "outputId": "156dc6e5-b728-4933-edff-11f00f8bb6d5"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "n_batch = len(train_dl)\n",
    "n_val_batch = len(val_dl)\n",
    "\n",
    "postfix = {}\n",
    "\n",
    "for epoch_index in range(1):\n",
    "\n",
    "    model.train()\n",
    "    with tqdm(total=int(0.1*n_batch)) as prog_bar:\n",
    "        for i, data in enumerate(train_dl):\n",
    "            prog_bar.update()\n",
    "            \n",
    "            inputs, attributes = data\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            recon, z_dist, prior_dist, z_tilde = model(inputs)\n",
    "            \n",
    "            loss = compute_loss(\n",
    "                inputs, recon, z_dist, prior_dist, z_tilde, attributes.cuda()\n",
    "            )\n",
    "\n",
    "            loss['loss'].backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            postfix.update({f\"train/{k}\": loss[k].detach().cpu().numpy() for k in loss})\n",
    "            \n",
    "            # for training, we calculate the metrics every 8 batches\n",
    "            if i % 8 == 0:\n",
    "\n",
    "                # Latte automatically move all data to CPU\n",
    "                # There is no need to call `.cpu()` here.\n",
    "                dami.reset()\n",
    "                dami.update(z_tilde, attributes)\n",
    "                train_metrics = dami.compute()\n",
    "                \n",
    "                # We only put the mean metrics over the attributes here on the progress bar for demonstration\n",
    "                postfix.update({f\"train/{k}\": train_metrics[k].mean().detach().cpu().numpy() for k in train_metrics})\n",
    "\n",
    "            prog_bar.set_postfix(postfix)\n",
    "            \n",
    "            if i > 0.1 * n_batch:\n",
    "                break\n",
    "\n",
    "        # reset cache for validation loop\n",
    "        dami.reset()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "    with tqdm(total=int(0.1*n_val_batch)) as prog_bar:\n",
    "        \n",
    "        for j, data in enumerate(val_dl):\n",
    "            \n",
    "            prog_bar.update()\n",
    "\n",
    "            inputs, attributes = data\n",
    "            inputs = inputs.cuda()\n",
    "\n",
    "            recon, z_dist, prior_dist, z_tilde = model(inputs)\n",
    "\n",
    "            loss = compute_loss(\n",
    "                inputs, recon, z_dist, prior_dist, z_tilde, attributes.cuda()\n",
    "            )\n",
    "\n",
    "            val_loss += loss['loss']\n",
    "\n",
    "            # use the entire validation set to compute metrics this time\n",
    "            dami.update(z_tilde, attributes)\n",
    "            \n",
    "            if j > 0.1 * n_val_batch:\n",
    "                break\n",
    "\n",
    "        # only compute once at the end of the validation loop \n",
    "        # using all validation batches\n",
    "        val_metrics = dami.compute()\n",
    "\n",
    "        print(f\"Validation loss: {val_loss/len(val_dl):3.2g}\")\n",
    "        for metric in val_metrics:\n",
    "            print(f\"Validation {metric}: {val_metrics[metric].numpy().mean():.3g}\")\n",
    "\n",
    "        # reset cache for the next train loop\n",
    "        dami.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atttribute_dict = {\n",
    "    \"thickness\": 0,\n",
    "    \"slant\": 1,\n",
    "    \"width\": 2,\n",
    "    \"height\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def interpolate_and_display(model, attribute):\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    \n",
    "    f = plt.figure(figsize=(16, 16))\n",
    "    ax = ImageGrid(\n",
    "        f, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=(11, 8),  # creates 2x2 grid of axes\n",
    "        axes_pad=0.1,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    inputs, _ = dataset.val_dataset[torch.randint(10000, (8,))]\n",
    "\n",
    "    dz = torch.zeros((16, 10))\n",
    "    dz[atttribute_dict[attribute], :] = torch.linspace(-2.0, 2.0, 10)\n",
    "\n",
    "    gen = model.interpolate(inputs.cuda(), dz.cuda())\n",
    "    gen = torch.sigmoid(gen)\n",
    "\n",
    "    for i in range(8):\n",
    "        ax[i].imshow(inputs.detach().cpu()[i, 0, :, :], cmap='summer')\n",
    "        for j in range(10):\n",
    "            ax[(j+1)*8+i].imshow(gen.detach().cpu()[i, 0, :, :, j], cmap='gray', vmin=0, vmax=1)\n",
    "    \n",
    "    for i in range(8*11):\n",
    "        ax[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_and_display(model, 'thickness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_and_display(model, 'slant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_and_display(model, 'width')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "mnist-lightning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26be1cc09bba403491747d0a20140373": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2fa0e4d346ea48c3a520ca8d9e014703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42e75f19949c48b1b0fd5ef711481334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "47c1be5335ad4f1e97b81dddda3a9d79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53ad6097a36c4f168993fa7e7c9c0256",
      "max": 1875,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6a2a135150974910b197f7b5d501a0f4",
      "value": 1875
     }
    },
    "53ad6097a36c4f168993fa7e7c9c0256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5970bb50ff844ec7acfd02ff68db8010": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fe4080d475a41319b3ea064267ab37b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2fa0e4d346ea48c3a520ca8d9e014703",
      "value": "epoch 1/10: 100%"
     }
    },
    "6a2a135150974910b197f7b5d501a0f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a399fb992244e4f95632df22228db02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5970bb50ff844ec7acfd02ff68db8010",
       "IPY_MODEL_47c1be5335ad4f1e97b81dddda3a9d79",
       "IPY_MODEL_84c042f3ceca4495be006145d97c81cf"
      ],
      "layout": "IPY_MODEL_26be1cc09bba403491747d0a20140373"
     }
    },
    "7fe4080d475a41319b3ea064267ab37b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84c042f3ceca4495be006145d97c81cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f85f8eec6ae4419873149fe830fa6a4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_42e75f19949c48b1b0fd5ef711481334",
      "value": " 1875/1875 [03:14&lt;00:00, 45.70it/s, train/loss=1.8e+02, train/DLIG=-0.00223, train/DMIG=-0.000701, train/MIG=-0.000605, train/XMIG=0.00731]"
     }
    },
    "9f85f8eec6ae4419873149fe830fa6a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
